---
title: "Empirical Hierarchical Model: Theory and Implementation"
date: last-modified
---

## Overview

The "Empirical" model (`model_empirical`) represents a departure from the physics-based models that rely on Finite Element Model (FEM) simulations. Instead of calibrating physical parameters ($E_1, E_2$, etc.) against simulation data, this model treats the experimental data as a direct hierarchical regression problem.

This approach is particularly useful when:

*   Likelihood of FEM mismatch is high.
*   We want to establish a baseline for pure data-driven performance.
*   Physical parameter identification is not the primary goal, but rather predictive performance for new experiments.

## Mathematical Formulation

### Core Regression

The foundational assumption is that extension ($y$) is linearly related to Load ($P$) through an effective slope ($\mu_{effective}$), modulated by the test angle ($\alpha$):

$$
y_{shear} = \mu_{effective} \cdot P \cdot \sin(\alpha) + \epsilon
$$

$$
y_{normal} = \mu_{effective} \cdot P \cdot \cos(\alpha) + \epsilon
$$

### Hierarchical Structure

To account for variability between different experiments (e.g., slight manufacturing differences, clamping variations), we introduce a hierarchical structure on the slope.

The effective slope for experiment $i$, denoted $\mu_i$, is defined as:

$$
\mu_i = \mu_{emulator} + b_i
$$

Where:

*   $\mu_{emulator}$: The global mean slope (or "emulator" slope) shared across all experiments.
*   $b_i$: The random effect (bias) specific to experiment $i$.

### Priors

The model places the following priors on the parameters:

*   **Global Slope**: LogNormal (or Normal) prior to ensure positivity.
    $$ \mu_{emulator} \sim \text{LogNormal}(\dots) $$

*   **Experiment Bias**: Modeled as deviations centered around zero.
    $$ b_i \sim \text{Normal}(0, \sigma_{b\_slope}^2) $$
    
    *   $\sigma_{b\_slope}$: The hyperparameter controlling the magnitude of inter-experiment variability. It typically has an Exponential or HalfNormal prior.

### Likelihood

The likelihood function assumes the observed data $y_{obs}$ follows a Multivariate Normal distribution centered on the model predictions:

$$
y_{obs} \sim \mathcal{N}(\boldsymbol{\mu}_{model}, \Sigma)
$$

The covariance matrix $\Sigma$ is diagonal (assuming independent measurement noise) with variance $\sigma^2$:

$$
\sigma^2 = \sigma_{measure}^2 \cdot P + \sigma_{constant}^2
$$

(Note: The default noise model is proportional to Load $P$).

## Implementation Details

### Code Structure

The model is implemented in `src/core/models.py` as `model_empirical`.

*   **Inputs**: It takes `input_xy_exp` (Load, Angle) and `data_exp` (Extension) as inputs. It *ignores* simulation inputs.
*   **Parameters**:
    *   `mu_emulator`: Global slope.
    *   `b_{i}_slope`: Bias for experiment $i$.
    *   `sigma_b_slope`: Variance of the bias.
    *   `sigma_measure`: Measurement noise scaling.

### JAX/NumPyro Implementation

The implementation uses a loop over the number of experiments to sample specific bias terms $b_i$ and compute the likelihood for that experiment's batch.

```python
# Simplified pseudocode
for i in range(num_exp):
    # Sample bias for this experiment
    b_i = numpyro.sample(f"b_{i+1}_slope", dist.Normal(0, sigma_b_slope))
    
    # Effective slope
    slope_eff = mu_emulator + b_i
    
    # Compute mean prediction
    mean_pred = slope_eff * load * trig_func(angle)
    
    # Likelihood
    numpyro.sample(f"obs_{i}", dist.MultivariateNormal(mean_pred, cov), obs=data)
```

### Configuration

To use this model, update `configs/default_config.py`:

```python
config = {
    "model_type": "model_empirical",
    "bias": {
        "add_bias_slope": True,  # Enables the hierarchical bias term
    },
    # ...
}
```

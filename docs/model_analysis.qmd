# Model Analysis: src/core/models.py



This document provides a detailed comparison of the three model definitions in `src/core/models.py` (`model`, `model_n`, and `model_n_hv`) and an in-depth analysis of the likelihood function, specifically for the multi-output case.

## 1. Model Comparison

The file contains three distinct model formulations. The key differences lie in their **parameterization strategy** (Centered vs. Non-Centered), **data handling** (Single vs. Multi-output), and **prior distributions**.

| Feature | `model` | `model_n` | `model_n_hv` |
| :--- | :--- | :--- | :--- |
| **Type** | Baseline | Reparameterized | Multi-Output Reparameterized |
| **Parameterization** | **Centered**<br>Parameters sampled directly from priors. | **Non-Centered**<br>Parameters sampled from $N(0,1)$ and scaled. | **Non-Centered**<br>Parameters sampled from $N(0,1)$ and scaled. |
| **Outputs** | Single (Implicitly Shear) | Single (Switchable S/N) | **Dual (Shear & Normal)** |
| **Mean Function** | $P \sin(\alpha)$ | $P \sin(\alpha)$ OR $P \cos(\alpha)$ | $P \sin(\alpha)$ AND $P \cos(\alpha)$ |
| **Priors** | Set A | Set A | **Set B (Updated)** |

### Detailed Prior Comparison

A critical difference is that `model_n_hv` uses a different set of physical priors compared to `model` and `model_n`.

| Parameter | `model` / `model_n` (Set A) | `model_n_hv` (Set B) | Change Analysis |
| :--- | :--- | :--- | :--- |
| **$E_1$** | $161,000 \pm 2,000$ | $165,000 \pm 6,050$ | **Mean $\uparrow$, Uncertainty $\uparrow \times 3$** |
| **$E_2$** | $11,380 \pm 100$ | $11,500 \pm 250$ | Mean $\uparrow$, Uncertainty $\uparrow$ |
| **$\nu_{12}$** | $0.32 \pm 0.01$ | $0.36 \pm 0.005$ | **Mean $\uparrow$, Uncertainty $\downarrow \times 0.5$** |
| **$\nu_{23}$** | $0.43 \pm 0.01$ | $0.41 \pm 0.01$ | Mean $\downarrow$ |
| **$G_{12}$** | $5,170 \pm 70$ | $5,000 \pm 80$ | Mean $\downarrow$ |

### Understanding `_n` Parameters

You may notice parameters with an `_n` suffix (e.g., `sigma_emulator_n`, `E_1_n`). These are **coupled** but not typically "clones" of their counterparts.

*   **`param_n`** is the **normalized** latent variable sampled by the MCMC (usually $\sim \mathcal{N}(0, 1)$).
*   **`param`** is the **physical** value derived from it: $\theta = \mu + \sigma \cdot \theta_n$ (for Normal) or similar transformations.

They contain the same information (one is a deterministic function of the other), but their plots show the distribution on different scales (Standard Normal vs. Physical Units). The `_n` plots are useful for checking convergence (should look like a "fuzzy caterpillar" around 0).

> [!NOTE]
> `model_n_hv` assumes significantly higher uncertainty for $E_1$ but tighter constraints on $\nu_{12}$.

### Hyperparameter Priors

In addition to the physical parameters, the model also estimates hyperparameters governing the Gaussian Process emulator and measurement noise. These are consistent across all models (though `model_n` and `model_n_hv` use reparameterization).

| Hyperparameter | Description | Prior Distribution |
| :--- | :--- | :--- |
| **$\mu_{emulator}$** | GP Mean Offset | $\mathcal{N}(0, 0.01)$ |
| **$\sigma_{emulator}$** | GP Amplitude | $\text{Exponential}(20.0)$ |
| **$\sigma_{measure}$** | Measurement Noise | $\text{Exponential}(100.0)$ |
| **$\lambda_P$** | Length Scale (Load) | $\text{LogNormal}(1.5, 0.5)$ |
| **$\lambda_\alpha$** | Length Scale (Angle) | $\text{LogNormal}(0.34, 0.5)$ |
| **$\lambda_{E1}$** | Length Scale ($E_1$) | $\text{LogNormal}(11.0, 0.5)$ |
| **$\lambda_{E2}$** | Length Scale ($E_2$) | $\text{LogNormal}(8.3, 0.5)$ |
| **$\lambda_{\nu12}$** | Length Scale ($\nu_{12}$) | $\text{LogNormal}(-0.80, 0.5)$ |
| **$\lambda_{\nu23}$** | Length Scale ($\nu_{23}$) | $\text{LogNormal}(-0.80, 0.5)$ |
| **$\lambda_{G12}$** | Length Scale ($G_{12}$) | $\text{LogNormal}(7.7, 0.5)$ |

*Note: The length scales $\lambda$ determine how quickly the model discrepancy changes with respect to changes in inputs or parameters. A small length scale implies a "wiggly" function, while a large one implies a smooth function.*

### Bias Modeling

The code also supports adding bias terms to account for systematic discrepancies in specific parameters. This is modeled hierarchically:

1.  **Global Scale Parameter**: A global scale $\sigma_b$ is sampled from an Exponential prior.
2.  **Experiment-Specific Bias**: For each experiment $i$, a specific bias $b_i$ is sampled from $\mathcal{N}(0, \sigma_b)$.
3.  **Application**:
    *   **$E_1$ Bias**: Added to the parameter samples for each experiment before emulation.
    *   **$\alpha$ Bias**: Added to the angle input for each experiment.

| Bias Type | Prior for Scale $\sigma_b$ | Implied Mean of Scale | Physical Interpretation |
| :--- | :--- | :--- | :--- |
| **$E_1$ Bias** ($b_{E1}$) | $\text{Exponential}(0.001)$ | $1,000 \text{ MPa}$ | Accounts for variability in stiffness between experiments. |
| **$\alpha$ Bias** ($b_{\alpha}$) | $\text{Exponential}(1 / \text{rad}(10^\circ))$ | $0.1745 \text{ rad} \approx 10^\circ$ | Accounts for misalignment of the specimen fibers. |

*Note: The rate parameter for $\alpha$ bias is $1 / (10 \cdot \frac{\pi}{180}) \approx 5.73$.*


## 2. Likelihood Analysis

All models use a **Gaussian Process (GP) Likelihood**. The observed data $\mathbf{y}$ is modeled as coming from a Multivariate Normal distribution:

$$ \mathbf{y} \sim \mathcal{N}(\boldsymbol{\mu}(\mathbf{x}, \theta), \mathbf{K}(\mathbf{x}, \theta) + \mathbf{\Sigma}_\epsilon) $$

Where:

*   $\boldsymbol{\mu}(\mathbf{x}, \theta)$: The physics-based mean function (Finite Element surrogate).
*   $\mathbf{K}(\mathbf{x}, \theta)$: The GP covariance matrix representing emulator uncertainty (bias).
*   $\mathbf{\Sigma}_\epsilon$: Measurement noise matrix. Note that the noise variance is **proportional to the load**: $\mathbf{\Sigma}_{ii} = \sigma_{measure}^2 \times P_i$.

*   $\mathbf{\Sigma}_\epsilon$: Measurement noise matrix. Note that the noise variance is **proportional to the load**: $\mathbf{\Sigma}_{ii} = \sigma_{measure}^2 \times P_i$.

### 2.1 Experimental Data Aggregation

> [!IMPORTANT]
> **Data Usage Note**: The current Bayesian analysis uses **averaged experimental data** for the likelihood calculation.

The raw experimental data contains measurements from three sensor positions (Left, Center, Right) for each load step. However, the current inference input ($\mathbf{y}$) is constructed by **averaging** these three values into a single mean extension per load step.

*   **Current State**: Likelihood is computed against the mean response: $\mathbf{y}_i = \frac{1}{3} (y_{i,L} + y_{i,C} + y_{i,R})$.
*   **Future Capability**: The code retains the raw positional data in the data loading pipeline (`data_loader.py` returns `_raw` arrays). This allows for future extensions where the likelihood could explicitly model the separate sensor outputs, potentially to capture asymmetric effects or sensor-specific noise.

### The `model_n_hv` Likelihood (Dual Output)

The `model_n_hv` function is unique because it performs **Multi-Output Inference**. It evaluates the likelihood of observing both Shear ($S$) and Normal ($N$) extensions simultaneously for a given set of material parameters.

#### A. Conditional Independence
The model assumes that given the true material parameters $\theta$ and the GP emulator state, the residuals for Shear and Normal data are independent. The total log-likelihood is the sum of the individual log-likelihoods:

$$ \log P(\mathbf{y}_h, \mathbf{y}_v | \theta) = \log P(\mathbf{y}_h | \theta) + \log P(\mathbf{y}_v | \theta) $$

In the code:
```python
# Sample Shear
mu_s, sigma_s = gp_predict(..., direction='h') # Shear (formerly Horizontal)
# Likelihood(Data_Shear | mu_s, sigma_s)

# Sample Normal
mu_n, sigma_n = gp_predict(..., direction='v') # Normal (formerly Vertical)
# Likelihood(Data_Normal | mu_n, sigma_n)
```

#### B. Physical Coupling (The "Why")
Although the likelihood statements are separate, the inference is **strongly coupled** via the shared parameters:

1.  **Shared Physics ($\theta$)**: The same $E_1, E_2, \dots$ must explain both the shear deformation ($H$) and the normal deformation ($V$).
    *   $\boldsymbol{\mu}_h = \text{Emulator}(\theta) \cdot P \sin(\alpha)$
    *   $\boldsymbol{\mu}_v = \text{Emulator}(\theta) \cdot P \cos(\alpha)$
    *   *Effect*: This constrains the parameter space significantly. A parameter set that fits $H$ well but $V$ poorly will have a low total likelihood.

2.  **Shared Emulator Structure ($\mathbf{K}$)**:
    *   The covariance matrix $\mathbf{K}$ is identical for both outputs.
    *   *Effect*: The model assumes the "smoothness" and "scale" of the model discrepancy is similar for both directions.

3.  **Shared Noise Model**:
    *   A single `sigma_measure` is used for both.
    *   *Effect*: Assumes similar sensor noise characteristics for both measurement types.

### 3. Code Implementation Details

#### Centered vs. Non-Centered Parameterization
**Centered (`model`)**:
```python
# Harder for MCMC to sample if geometry is complex
x = numpyro.sample("x", dist.Normal(mu, sigma))
```

**Non-Centered (`model_n`, `model_n_hv`)**:
```python
# Easier for MCMC (Standard Normal)
x_n = numpyro.sample("x_n", dist.Normal(0, 1))
x = mu + sigma * x_n
numpyro.deterministic("x", x)
```
*Why?* This decouples the dependency between the mean/scale and the sample value, avoiding "funnel" pathologies in the posterior geometry, leading to more efficient sampling.

#### Directional Logic
*   **`model`**: Hardcoded to $\sin(\alpha)$ (Shear).
*   **`model_n`**: Checks `if direction == 'h'` to choose between $\sin$ and $\cos$.
*   **`model_n_hv`**: Computes **both** and samples **both**.

```python
# model_n_hv logic
mean_vector_h = mean_emulator * input_xy[:,0] * jnp.sin(input_xy[:,1])
mean_vector_v = mean_emulator * input_xy[:,0] * jnp.cos(input_xy[:,1])
```

### Additive Noise Model (Optional)

By default, the codebase assumes a measurement noise variance proportional to load (`noise_model: "proportional"`):
$$ \sigma_{\text{noise}}^2 = \sigma_{\text{measure}}^2 \cdot P $$
where $P$ is the load.

An optional **Additive Noise Model** (`noise_model: "additive"`) is available to handle cases where noise persists at zero load:
$$ \sigma_{\text{noise}}^2 = \sigma_{\text{measure}}^2 \cdot P + \sigma_{\text{base}}^2 $$

-   **$\sigma_{\text{base}}$**: Represents the "base" or "background" noise variance that is independent of load.
-   **Configuration**: This can be enabled in `defaults_config.py` by setting `"data": {"noise_model": "additive"}`.
-   **Prior**: The prior for $\sigma_{\text{base}}$ can be configured under `"priors" -> "simga_measure_base"`.

### Understanding Priors for Additive Noise

When using the additive noise model, you might notice that `sigma_measure` (proportional) and `sigma_measure_base` (constant) can have similar numerical priors (e.g., `Exponential(100)`), despite representing different physical concepts. This is due to the units involved in the variance equation:

$$ \sigma_{\text{total}}^2 = \sigma_{\text{measure}}^2 \cdot P + \sigma_{\text{base}}^2 $$

1.  **$\sigma_{\text{base}}$ (Base Noise)**:
    *   This term has the **same units as the data** (e.g., mm).
    *   If measurement noise is small (e.g., $\approx 0.01$ mm), then $\sigma_{\text{base}} \approx 0.01$.
    *   A prior like `Exponential(100)` implies a mean of $1/100 = 0.01$, which is appropriate for this small scale.

2.  **$\sigma_{\text{measure}}$ (Proportional Term)**:
    *   This term is multiplied by Load ($P$).
    *   For the units to be consistent ($\text{mm}^2$), $\sigma_{\text{measure}}^2$ must have units of $\text{mm}^2 / \text{kN}$.
    *   If the typical Load $P \approx 10$ kN, then $\sigma_{\text{measure}}^2 \cdot 10 \approx \sigma_{\text{base}}^2$.
    *   This implies $\sigma_{\text{measure}}$ should be smaller than $\sigma_{\text{base}}$ by a factor of $\sqrt{P}$.
    *   However, if we use a loose prior (like `Exponential(1.0)` with mean 1.0), the model may overestimate noise. Using a tighter prior like `Exponential(100)` for this term as well acts as a conservative regularizer, ensuring the proportional noise component remains physically realistic.

### Constant Variance Noise Model

A third option is the **Constant Variance Noise Model** (`noise_model: "constant"`):

$$ \sigma_{\text{noise}}^2 = \sigma_{\text{constant}}^2 $$

-   **Assumption**: The noise variance is practically constant and does not depend on the load mechanism. This is effectively a standard homoscedastic Gaussian noise model.
-   **Configuration**: Set `"data": {"noise_model": "constant"}` in `default_config.py`.
-   **Prior**: The prior for $\sigma_{\text{constant}}$ is configured under `"priors" -> "sigma_constant"`.


# Model Analysis: src/core/models.py



This document provides a detailed analysis of the two active model definitions in `src/core/models.py` (`model_n` and `model_n_hv`) and the likelihood function.

## 1. Model Comparison

The file contains three distinct model formulations. The key differences lie in their **parameterization strategy** (Centered vs. Non-Centered), **data handling** (Single vs. Multi-output), and **prior distributions**.

| Feature | `model_n` | `model_n_hv` | `model_hierarchical` |
| :--- | :--- | :--- | :--- |
| **Type** | Reparameterized | Multi-Output Reparameterized | Hierarchical Multi-Output |
| **Parameterization** | **Non-Centered** | **Non-Centered** | **Non-Centered** + Latent Mapping |
| **Outputs** | Single | **Dual (S & N)** | **Dual (S & N)** |
| **Mean Function** | $P \sin(\alpha)$ OR $P \cos(\alpha)$ | $P \sin(\alpha)$ AND $P \cos(\alpha)$ | $P \sin(\alpha) + \epsilon_i$ ... |
| **Priors** | Set A | Set A | Set A + $\sigma_{latent}$ |

### Detailed Prior Comparison

A critical difference is that `model_n_hv` uses a different set of physical priors compared to `model` and `model_n`.

| Parameter | Original / Legacy (Set A) | Current (Set B) | Change Analysis |
| :--- | :--- | :--- | :--- |
| **$E_1$** | $161,000 \pm 2,000$ | $154,900 \pm 5,050$ | **Mean $\downarrow$, Uncertainty $\uparrow \times 2.5$** |
| **$E_2$** | $11,380 \pm 100$ | $10,285 \pm 650$ | Mean $\downarrow$, Uncertainty $\uparrow$ |
| **$\nu_{12}$** | $0.32 \pm 0.01$ | $0.33 \pm 0.015$ | Mean $\uparrow$ |
| **$\nu_{23}$** | $0.43 \pm 0.01$ | $0.435 \pm 0.0125$ | Mean $\approx$ |
| **$G_{12}$** | $5,170 \pm 70$ | $5,115 \pm 98$ | Mean $\downarrow$ |

### Understanding `_n` Parameters

You may notice parameters with an `_n` suffix (e.g., `sigma_emulator_n`, `E_1_n`). These are **coupled** but not typically "clones" of their counterparts:

*   **`param_n`** is the **normalized** latent variable sampled by the MCMC (usually $\sim \mathcal{N}(0, 1)$).
*   **`param`** is the **physical** value derived from it: $\theta = \mu + \sigma \cdot \theta_n$ (for Normal) or similar transformations.

They contain the same information (one is a deterministic function of the other), but their plots show the distribution on different scales (Standard Normal vs. Physical Units). The `_n` plots are useful for checking convergence (should look like a "fuzzy caterpillar" around 0).

> [!NOTE]
> `model_n_hv` assumes significantly higher uncertainty for $E_1$ but tighter constraints on $\nu_{12}$.

### Hyperparameter Priors

In addition to the physical parameters, the model also estimates hyperparameters governing the Gaussian Process emulator and measurement noise. These are consistent across all models (though `model_n` and `model_n_hv` use reparameterization).

| Hyperparameter | Description | Prior Distribution |
| :--- | :--- | :--- |
| **$\mu_{emulator}$** | GP Mean Offset | $\mathcal{N}(0, 0.01)$ |
| **$\sigma_{emulator}$** | GP Amplitude | $\text{Exponential}(20.0)$ |
| **$\sigma_{measure}$** | Measurement Noise | $\text{Exponential}(100.0)$ |
| **$\lambda_P$** | Length Scale (Load) | $\text{LogNormal}(1.5, 0.5)$ |
| **$\lambda_\alpha$** | Length Scale (Angle) | $\text{LogNormal}(0.34, 0.5)$ |
| **$\lambda_{E1}$** | Length Scale ($E_1$) | $\text{LogNormal}(11.0, 0.5)$ |
| **$\lambda_{E2}$** | Length Scale ($E_2$) | $\text{LogNormal}(8.3, 0.5)$ |
| **$\lambda_{\nu12}$** | Length Scale ($\nu_{12}$) | $\text{LogNormal}(-0.80, 0.5)$ |
| **$\lambda_{\nu23}$** | Length Scale ($\nu_{23}$) | $\text{LogNormal}(-0.80, 0.5)$ |
| **$\lambda_{G12}$** | Length Scale ($G_{12}$) | $\text{LogNormal}(7.7, 0.5)$ |

*Note: The length scales $\lambda$ determine how quickly the model discrepancy changes with respect to changes in inputs or parameters. A small length scale implies a "wiggly" function, while a large one implies a smooth function.*

### Bias Modeling

The code also supports adding bias terms to account for systematic discrepancies in specific parameters. This is modeled hierarchically:

1.  **Global Scale Parameter**: A global scale $\sigma_b$ is sampled from an Exponential prior.
2.  **Experiment-Specific Bias**: For each experiment $i$, a specific bias $b_i$ is sampled from $\mathcal{N}(0, \sigma_b)$.
3.  **Application**:
    *   **$E_1$ Bias**: Added to the parameter samples for each experiment before emulation.
    *   **$\alpha$ Bias**: Added to the angle input for each experiment.

| Bias Type | Prior for Scale $\sigma_b$ | Implied Mean of Scale | Physical Interpretation |
| :--- | :--- | :--- | :--- |
| **$E_1$ Bias** ($b_{E1}$) | $\text{Exponential}(0.001)$ | $1,000 \text{ MPa}$ | Accounts for variability in stiffness between experiments. |
| **$\alpha$ Bias** ($b_{\alpha}$) | $\text{Exponential}(1 / \text{rad}(10^\circ))$ | $0.1745 \text{ rad} \approx 10^\circ$ | Accounts for misalignment of the specimen fibers. |

*Note: The rate parameter for $\alpha$ bias is $1 / (10 \cdot \frac{\pi}{180}) \approx 5.73$.*


## 2. Likelihood Analysis

All models use a **Gaussian Process (GP) Likelihood**. The observed data $\mathbf{y}$ is modeled as coming from a Multivariate Normal distribution:

$$ \mathbf{y} \sim \mathcal{N}(\boldsymbol{\mu}(\mathbf{x}, \theta), \mathbf{K}(\mathbf{x}, \theta) + \mathbf{\Sigma}_\epsilon) $$

Where:

*   $\boldsymbol{\mu}(\mathbf{x}, \theta)$: The physics-based mean function (Finite Element surrogate).
*   $\mathbf{K}(\mathbf{x}, \theta)$: The GP covariance matrix representing emulator uncertainty (bias).
*   $\mathbf{\Sigma}_\epsilon$: Measurement noise matrix. Note that the noise variance is **proportional to the load**: $\mathbf{\Sigma}_{ii} = \sigma_{measure}^2 \times P_i$.

### 2.1 Experimental Data Aggregation

> [!IMPORTANT]
> **Data Usage Note**: The current Bayesian analysis uses **averaged experimental data** for the likelihood calculation.

The raw experimental data contains measurements from three sensor positions (Left, Center, Right) for each load step. However, the current inference input ($\mathbf{y}$) is constructed by **averaging** these three values into a single mean extension per load step.

*   **Current State**: Likelihood is computed against the mean response: $\mathbf{y}_i = \frac{1}{3} (y_{i,L} + y_{i,C} + y_{i,R})$.
*   **Future Capability**: The code retains the raw positional data in the data loading pipeline (`data_loader.py` returns `_raw` arrays). This allows for future extensions where the likelihood could explicitly model the separate sensor outputs, potentially to capture asymmetric effects or sensor-specific noise.

### The `model_n_hv` Likelihood (Dual Output)

The `model_n_hv` function is unique because it performs **Multi-Output Inference**. It evaluates the likelihood of observing both Shear ($S$) and Normal ($N$) extensions simultaneously for a given set of material parameters.

#### A. Conditional Independence
The model assumes that given the true material parameters $\theta$ and the GP emulator state, the residuals for Shear and Normal data are independent. The total log-likelihood is the sum of the individual log-likelihoods:

$$ \log P(\mathbf{y}_h, \mathbf{y}_v | \theta) = \log P(\mathbf{y}_h | \theta) + \log P(\mathbf{y}_v | \theta) $$

In the code:
```python
# Sample Shear
mu_s, sigma_s = gp_predict(..., direction='h') # Shear (formerly Horizontal)
# Likelihood(Data_Shear | mu_s, sigma_s)

# Sample Normal
mu_n, sigma_n = gp_predict(..., direction='v') # Normal (formerly Vertical)
# Likelihood(Data_Normal | mu_n, sigma_n)
```

#### B. Physical Coupling (The "Why")
Although the likelihood statements are separate, the inference is **strongly coupled** via the shared parameters:

1.  **Shared Physics ($\theta$)**: The same $E_1, E_2, \dots$ must explain both the shear deformation ($H$) and the normal deformation ($V$).
    *   $\boldsymbol{\mu}_h = \text{Emulator}(\theta) \cdot P \sin(\alpha)$
    *   $\boldsymbol{\mu}_v = \text{Emulator}(\theta) \cdot P \cos(\alpha)$
    *   *Effect*: This constrains the parameter space significantly. A parameter set that fits $H$ well but $V$ poorly will have a low total likelihood.

2.  **Shared Emulator Structure ($\mathbf{K}$)**:
    *   The covariance matrix $\mathbf{K}$ is identical for both outputs.
    *   *Effect*: The model assumes the "smoothness" and "scale" of the model discrepancy is similar for both directions.

3.  **Shared Noise Model**:
    *   A single `sigma_measure` is used for both.
    *   *Effect*: Assumes similar sensor noise characteristics for both measurement types.

### 3. Code Implementation Details

### Centered vs. Non-Centered Parameterization

The codebase exclusively uses **Non-Centered parameterization** to ensure sampling efficiency.

**Non-Centered (`model_n`, `model_n_hv`)**:
```python
# Easier for MCMC (Standard Normal)
x_n = numpyro.sample("x_n", dist.Normal(0, 1))
x = mu + sigma * x_n
numpyro.deterministic("x", x)
```
*Why?* This decouples the dependency between the mean/scale and the sample value, avoiding "funnel" pathologies in the posterior geometry, leading to more efficient sampling.

#### Directional Logic

*   **`model`**: Hardcoded to $\sin(\alpha)$ (Shear).
*   **`model_n`**: Checks `if direction == 'h'` to choose between $\sin$ and $\cos$.
*   **`model_n_hv`**: Computes **both** and samples **both**.

```python
# model_n_hv logic
mean_vector_h = mean_emulator * input_xy[:,0] * jnp.sin(input_xy[:,1])
mean_vector_v = mean_emulator * input_xy[:,0] * jnp.cos(input_xy[:,1])
```

### Proportional Noise Model (Default)

By default, the codebase assumes a measurement noise variance proportional to load (`noise_model: "proportional"`):
$$ \sigma_{\text{noise}}^2 = \sigma_{\text{measure}}^2 \cdot P $$
where $P$ is the load.

This model assumes **heteroscedasticity**: as the load increases, the physical variability (and thus error) increases naturally. This is consistent with many mechanical testing scenarios where error is a percentage of the signal.

### Additive Noise Model (Optional)

An optional **Additive Noise Model** (`noise_model: "additive"`) is available to handle cases where noise persists at zero load:
$$ \sigma_{\text{noise}}^2 = \sigma_{\text{measure}}^2 \cdot P + \sigma_{\text{base}}^2 $$

-   **$\sigma_{\text{base}}$**: Represents the "base" or "background" noise variance that is independent of load.
-   **Configuration**: This can be enabled in `defaults_config.py` by setting `"data": {"noise_model": "additive"}`.
-   **Prior**: The prior for $\sigma_{\text{base}}$ can be configured under `"priors" -> "simga_measure_base"`.

### Understanding Priors for Additive Noise

When using the additive noise model, you might notice that `sigma_measure` (proportional) and `sigma_measure_base` (constant) can have similar numerical priors (e.g., `Exponential(100)`), despite representing different physical concepts. This is due to the units involved in the variance equation:

$$ \sigma_{\text{total}}^2 = \sigma_{\text{measure}}^2 \cdot P + \sigma_{\text{base}}^2 $$

1.  **$\sigma_{\text{base}}$ (Base Noise)**:
    *   This term has the **same units as the data** (e.g., mm).
    *   If measurement noise is small (e.g., $\approx 0.01$ mm), then $\sigma_{\text{base}} \approx 0.01$.
    *   A prior like `Exponential(100)` implies a mean of $1/100 = 0.01$, which is appropriate for this small scale.

2.  **$\sigma_{\text{measure}}$ (Proportional Term)**:
    *   This term is multiplied by Load ($P$).
    *   For the units to be consistent ($\text{mm}^2$), $\sigma_{\text{measure}}^2$ must have units of $\text{mm}^2 / \text{kN}$.
    *   If the typical Load $P \approx 10$ kN, then $\sigma_{\text{measure}}^2 \cdot 10 \approx \sigma_{\text{base}}^2$.
    *   This implies $\sigma_{\text{measure}}$ should be smaller than $\sigma_{\text{base}}$ by a factor of $\sqrt{P}$.
    *   However, if we use a loose prior (like `Exponential(1.0)` with mean 1.0), the model may overestimate noise. Using a tighter prior like `Exponential(100)` for this term as well acts as a conservative regularizer, ensuring the proportional noise component remains physically realistic.

### Constant Variance Noise Model

A third option is the **Constant Variance Noise Model** (`noise_model: "constant"`):

$$ \sigma_{\text{noise}}^2 = \sigma_{\text{constant}}^2 $$

-   **Assumption**: The noise variance is practically constant and does not depend on the load mechanism. This is effectively a standard homoscedastic Gaussian noise model.
-   **Configuration**: Set `"data": {"noise_model": "constant"}` in `default_config.py`.

-   **Prior**: The prior for $\sigma_{\text{constant}}$ is configured under `"priors" -> "sigma_constant"`.

## 3. Simulated Data Integration

The Bayesian model (`model_n_hv`) treats the simulated data as "ground truth" observations of the physics, which constrain the Gaussian Process (GP) emulator. While the experimental data drives the calibration of unknown parameters, the simulated data teaches the model the relationship between inputs and outputs.

### 3.1 Data Flow

The following three files are critical for this integration:

#### 1. Control Inputs (`input_load_angle_sim.txt`)
*   **Content**: 2 columns representing $[P, \alpha]$ (Load, Angle).
*   **Action**: Loaded directly into `input_xy_sim`.
*   **Role**: These are concatenated with the experimental inputs to form the full $X$ matrix for the GP covariance function.
    $$ \mathbf{X}_{total} = \begin{bmatrix} P_{exp} & \alpha_{exp} \\ P_{sim} & \alpha_{sim} \end{bmatrix} $$

#### 2. Physical Parameters (`input_theta_sim.txt`)
*   **Content**: 5 columns representing $[E_1, E_2, \nu_{12}, \nu_{23}, G_{12}]$.
*   **Action**: Loaded into `input_theta_sim`.
*   **Role**: The GP needs a parameter vector $\theta$ for *every* data point.
    *   **For Experimental points**: The model uses the **sampled/inferred** parameters $\theta_{sampled}$. These are repeated (tiled) for every experimental row.
    *   **For Simulated points**: The model uses the **known/fixed** parameters from this file.
    
    $$ \Theta_{total} = \begin{bmatrix} \text{Tile}(\theta_{sampled}) \\ \Theta_{sim\_fixed} \end{bmatrix} $$

#### 3. Output Response (`data_extension_sim.txt`)
*   **Content**: Multiple columns representing sensors (Left, Center, Right).
*   **Action**: These are **averaged immediately** upon loading (`.mean(axis=1)`) to create a single scalar extension value per simulation point.
*   **Role**: These averaged values become the "observed" $Y$ values for the simulation part of the dataset. They are concatenated with the experimental observations.

### 3.2 The Joint Gaussian Process

By training on this combined dataset, the model learns a single GP function $f(P, \alpha, \theta) \approx \text{Extension}$.
*   The **experimental data** provides points where $\theta$ is *unknown*. The inference algorithm searches for the $\theta$ that makes the experimental observations likely under the GP function learned from the simulation.

### 3.3 Visualizing the Parameter Matrix (Tiling)

A key physical assumption is that **all experimental data points** come from the same physical specimen (or identical specimens), so they share the exact same material properties ($E_1, E_2$, etc.). To evaluate the Gaussian Process, the model constructs an input matrix where *every single row* corresponds to a data point.

Since $\theta$ is constant for all experiments in a single MCMC step, the model **tiles (repeats)** that single sampled $\theta^*$ vector into every experimental row.

$$
\text{Input}_{\theta} = 
\begin{bmatrix} 
\color{blue}{E_1^*} & \color{blue}{E_2^*} & \dots & \color{blue}{G_{12}^*} \\
\color{blue}{E_1^*} & \color{blue}{E_2^*} & \dots & \color{blue}{G_{12}^*} \\
\vdots & \vdots & \ddots & \vdots \\
\color{blue}{E_1^*} & \color{blue}{E_2^*} & \dots & \color{blue}{G_{12}^*} \\
\hline
\color{gray}{E_{1,sim}^{(1)}} & \color{gray}{E_{2,sim}^{(1)}} & \dots & \color{gray}{G_{12,sim}^{(1)}} \\
\color{gray}{E_{1,sim}^{(2)}} & \color{gray}{E_{2,sim}^{(2)}} & \dots & \color{gray}{G_{12,sim}^{(2)}} \\
\vdots & \vdots & \ddots & \vdots
\end{bmatrix}
\begin{matrix}
\leftarrow \text{Row 1 (Exp Data Point 1)} \\
\leftarrow \text{Row 2 (Exp Data Point 2)} \\
\vdots \\
\leftarrow \text{Row } N_{exp} \\
\\
\leftarrow \text{Row } N_{exp}+1 \text{ (Sim Data Point 1)} \\
\leftarrow \text{Row } N_{exp}+2 \text{ (Sim Data Point 2)} \\
\vdots
\end{matrix}
$$

*   **Top Section (Blue rows)**: These values change **every MCMC step** as the sampler proposes new candidates for the true material properties.
*   **Bottom Section (Gray rows)**: These values are **fixed** (loaded from `input_theta_sim.txt`). They anchor the physics of the model.

## 4. Covariance Structure Analysis

This section details the mathematical structure of the covariance matrices used in the likelihood and posterior predictions.

### 4.1 The Total Data Covariance $K_{obs,obs}$

The Gaussian Process covariance matrix (often called the Gram matrix) is constructed over the **joint** set of experimental and simulated data points. Its structure can be visualized as a block matrix:

$$
\mathbf{K}_{obs,obs} = 
\begin{bmatrix}
\mathbf{K}_{EE} & \mathbf{K}_{ES} \\
\mathbf{K}_{SE} & \mathbf{K}_{SS}
\end{bmatrix}
$$

*   **$\mathbf{K}_{EE}$**: Covariance between experimental points. Depends on the distance between inputs $(Load_{exp}, \theta^*)$ and itself.
*   **$\mathbf{K}_{SS}$**: Covariance between simulation points. Depends on fixed inputs from simulation files.
*   **$\mathbf{K}_{ES}$**: Cross-covariance between experimental and simulated points. This term is critical; it represents how much the simulation points "inform" the experimental predictions.

#### Understanding $K_{ES}$: The Cross-Covariance Assumption

The cross-covariance $\mathbf{K}_{ES}$ is **not assumed to be zero** — it is computed using the **same Squared Exponential (SE) kernel** as all other covariance blocks:

$$
K(x_i, x_j) = \sigma_{emulator}^2 \cdot \exp\left( -d_{xy} - d_{\theta} \right)
$$

where:

*   $d_{xy} = \sum_k \left( \frac{x_{xy,i}^k - x_{xy,j}^k}{\lambda_{xy}^k} \right)^2$ — squared distance in (Load, Angle) space
*   $d_\theta = \sum_k \left( \frac{\theta_i^k - \theta_j^k}{\lambda_\theta^k} \right)^2$ — squared distance in ($E_1$, $E_2$, $\nu_{12}$, $\nu_{23}$, $G_{12}$) space

When computing $K_{ES}$, the kernel evaluates the distance between:

*   **Experimental point** $i$: $(P_i^{exp}, \alpha_i^{exp}, \theta^*)$ — where $\theta^*$ is the **sampled** (inferred) material parameters
*   **Simulated point** $j$: $(P_j^{sim}, \alpha_j^{sim}, \theta_j^{sim})$ — where $\theta_j^{sim}$ is **fixed** from the simulation design

> [!IMPORTANT]
> **Key insight**: If the inferred $\theta^*$ is *close* to a simulation design point $\theta_j^{sim}$ (and the loads/angles are similar), then $K_{ES}[i,j]$ will be **large** — meaning that simulation point strongly informs the experimental prediction.

In essence, $K_{ES}$ encodes "how similar is the candidate material parameter $\theta^*$ to the parameters used in each simulation?" This is the mechanism by which the simulation data **informs** predictions at the experiment. The closer the sampled $\theta^*$ is to one of the simulation grid points $\theta_j^{sim}$, the more the GP "trusts" that simulation's output for predicting the experimental extension.

### 4.2 Measurement Noise Matrix $\Sigma_\epsilon$

The noise handling is distinct for experimental vs. simulated data. The total noise covariance matrix is diagonal:

$$
\mathbf{\Sigma}_\epsilon = 
\begin{bmatrix}
\mathbf{D}_{exp} & \mathbf{0} \\
\mathbf{0} & \mathbf{0}
\end{bmatrix}
+ \mathbf{J}_{jitter}
$$

*   **$\mathbf{D}_{exp}$**: Diagonal matrix of experimental measurement noise.
    *   For Proportional Noise: $D_{ii} = \sigma_{measure}^2 \times P_i$

*   **Simulation Noise**: Effectively **zero**. We assume the Finite Element simulation is a deterministic "truth" of the physics model (no measurement error).

*   **$\mathbf{J}_{jitter}$**: A tiny constant ($10^{-6}$) added to the entire diagonal for numerical stability (Cholesky decomposition).

### 4.3 Posterior Covariance $\Sigma_{post}$

During prediction (`posterior_predict`), we compute the covariance of the test points conditioned on the observed data. This uses the standard GP formula:

$$
\mathbf{\Sigma}_{post} = \mathbf{K}_{**} - \mathbf{K}_{*f} (\mathbf{K}_{obs,obs} + \mathbf{\Sigma}_\epsilon)^{-1} \mathbf{K}_{f*}
$$

Where:

*   $\mathbf{K}_{**}$: Prior covariance of the test points (the inherent uncertainty of the GP kernel).
*   $\mathbf{K}_{*f}$: Covariance between test points and all training data (Exp + Sim).
*   The term $(\mathbf{K}_{obs,obs} + \mathbf{\Sigma}_\epsilon)^{-1}$ acts as the "information gain" from the data, reducing the uncertainty.

> [!NOTE]
> Because $\mathbf{\Sigma}_\epsilon$ is zero for the simulation block, the GP is forced to pass very close to the simulation means (interpolating them), while it allows "slack" around the experimental points proportional to the measurement noise.

## Appendix: Legacy Design Decision - Dropping Centered Parameterization

Based on literature regarding Hamiltonian Monte Carlo and NUTS, the centered `model` was dropped in favor of the non-centered (reparameterized) versions (`model_n` / `model_n_hv`).

**Reasoning:**

1.  **The "Funnel" Problem**:
    *   **Literature**: In hierarchical models (like Gaussian Processes with learnable length scales), centered parameterizations ($\theta \sim N(\mu, \sigma)$) essentially create a geometry known as "Neal's Funnel."
    *   **The Issue**: The curvature of the posterior distribution changes drastically depending on the value of the hyperparameters. NUTS (the sampler used by NumPyro) struggles to explore the "neck" of this funnel, leading to divergences and very low Effective Sample Sizes (ESS).
    *   **Solution**: Non-centered parameterization (sampling $\theta_{raw} \sim N(0,1)$ and scaling it) "whitens" the geometry, making it much easier for the sampler to navigate.

2.  **Efficiency**:
    *   For the type of GP inference performed here (learning separate length scales and variances), the non-centered implementations (`model_n`) will almost always converge faster and produce more reliable chains than the centered `model`.


## 5. Hierarchical Model (`model_hierarchical`)

The `model_hierarchical` function expands on `model_n_hv` by introducing a **Latent Variable** for each specimen to account for specimen-specific variability that systematic trends (`theta`) cannot capture.

### 5.1 Mathematical Formulation

For each experiment (specimen) $i$:

1.  Sample a latent offset:
    $$ \epsilon_i \sim \mathcal{N}(0, \sigma_{\text{latent}}^2) $$
    where $\sigma_{\text{latent}} \sim \text{LogNormal}(-5, 1.0)$ (prior mean $\approx 0.01$).

2.  Modify the Mean Function:
    $$ \boldsymbol{\mu}_{total}(\mathbf{x}_i, \theta) = \boldsymbol{\mu}_{physics}(\mathbf{x}_i, \theta) + \epsilon_i $$
    
    This effectively shifts the entire predicted curve (Strain vs Load) up or down for that specific specimen.

### 5.2 Data Expansion and Performance

To implement this correctly, the Gaussian Process must treat "Experiment $i$" as distinct from "Experiment $j$" even if they are at the same angle, because they have different $\epsilon_i$.

*   **Implementation**: This is achieved by explicitly tracking experiment IDs.
*   **Data Expansion**: The experimental dataset is expanded by a factor of 3 (because each experiment has 3 simultaneous sensor readings that share the same $\epsilon_i$).
    *   Standard Model: $N \approx 200$ points.
    *   Hierarchical Model: $N \approx 600$ points.
*   **Computational Cost**: Since GP inference scales as $O(N^3)$, increasing $N$ by $3\times$ increases computational cost by $3^3 = 27\times$ (theoretically), though practically observed as ~12-13x due to optimizations and simulation data being constant.

### 5.3 Motivation

Standard models assume that all scatter around the mean prediction is independent measurement noise ($\Sigma_\epsilon$). However, real composite specimens often show **systematic offsets**: one specimen might be consistently stiffer than another due to fiber volume fraction variations or curing differences.

The hierarchical model disentangles:
*   **Measurement Noise** ($\sigma_{measure}$): Point-to-point scatter (jitter).
*   **Latent Offset** ($\sigma_{latent}$): Specimen-to-specimen scatter (shift).

This results in more accurate parameter estimates ($\theta$) by preventing specimen-specific outliers from skewing the global mean.

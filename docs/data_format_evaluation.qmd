# Data Format Strategy: NetCDF & ArviZ

## 1. The Chosen Standard: NetCDF & ArviZ

The project has standardized on **NetCDF** as the storage format and **[ArviZ](https://arviz-devs.github.io/arviz/)** as the backend for handling Bayesian inference results.

This decision was made to leverage the robust `InferenceData` structure, which provides significant advantages over raw array storage:

*   **Structured Metadata**: Instead of flat arrays, data is organized into labeled groups (`posterior`, `posterior_predictive`, `observed_data`).
*   **Labeled Dimensions**: Dimensions are explicitly named (e.g., `experiment`, `sensor_location`, `draw`) rather than being integer indices. This prevents "off-by-one" errors and makes array slicing intuitive.
*   **Built-in Diagnostics**: ArviZ provides one-line commands for essential MCMC diagnostics like Effective Sample Size (`az.ess`), Gelman-Rubin convergence (`az.rhat`), and summary statistics (`az.summary`).
*   **Rich Visualization**: Access to a high-quality suite of Bayesian visualization tools (`az.plot_trace`, `az.plot_forest`, `az.plot_ppc`) that work out-of-the-box with the data structure.

## 2. Why HDF5 Was Abandoned

Earlier versions of this workflow utilized raw **HDF5** (`h5py`) files. This approach was abandoned due to several critical friction points:

*   **Lack of Semantics**: HDF5 stores data effectively but does not inherently capture *what* the data represents (e.g., chains vs. draws). This required manual tracking of array shapes and indices.
*   **Boilerplate Overhead**: Performing simple tasks (like calculating the mean of a posterior parameter across chains) required writing custom iteration loops and array manipulation code.
*   **Fragility**: Any change in the model structure (e.g., adding a new parameter) often broke the custom loading scripts, whereas ArviZ handles schema changes more gracefully.
*   **Visualization Effort**: Creating trace plots or posterior density plots required writing raw Matplotlib code from scratch for every new model iteration.

Moving to ArviZ eliminated this technical debt, allowing the codebase to focus on modeling physics rather than managing data structures.

## 3. Implementation

The integration is handled seamlessly in the analysis scripts.

### Saving Results (`analyze.py` / MCMC Runner)

When inference is complete, the `numpyro` MCMC object is converted directly to `InferenceData` and saved:

```python
import arviz as az

# Convert NumPyro MCMC object to ArviZ InferenceData
idata = az.from_numpyro(mcmc)

# Save to robust NetCDF format
az.to_netcdf(idata, "results_mcmc/model_results.nc")
```

### Loading & Analyzing

The analysis workflow loads this single file to access all aspects of the model run:

```python
import arviz as az

# Load the full inference context
idata = az.from_netcdf("results_mcmc/model_results.nc")

# Example: Check convergence
print(az.summary(idata))

# Example: Plot posterior densities for stiffness parameters
az.plot_trace(idata, var_names=["E_1", "E_2"])
```

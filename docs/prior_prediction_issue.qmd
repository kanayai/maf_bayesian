---
title: "Gaussian Process Prediction Theory"
author: "Technical Reference"
date: "2025-12-08"
format:
  html:
    toc: true
    code-fold: false
    number-sections: true
---

# Gaussian Process Posterior Prediction

## Standard GP Formulation

Given a Gaussian Process emulator with:

- Training data: $(X_{train}, y_{train})$ where $y_{train} = f(X_{train}) + \varepsilon$
- Test points: $X_{test}$
- Kernel function: $k(x, x')$ with hyperparameters $\theta$
- Observation noise: $\varepsilon \sim \mathcal{N}(0, \sigma^2_{noise})$

The **posterior predictive distribution** at test points is:

$$
p(y_{test} | X_{test}, X_{train}, y_{train}) = \mathcal{N}(\mu_{post}, \Sigma_{post})
$$

where:

$$
\mu_{post} = K_{test,train} (K_{train,train} + \sigma^2_{noise}I)^{-1} y_{train}
$$

$$
\Sigma_{post} = K_{test,test} - K_{test,train} (K_{train,train} + \sigma^2_{noise}I)^{-1} K_{train,test}
$$

# GP Realizations via Cholesky Decomposition

## Why Cholesky Sampling?

To obtain a **realization** (sample path) from the GP posterior, we need to preserve the **spatial correlation structure** encoded in $\Sigma_{post}$.

**Incorrect approach (destroys correlation):**
$$
y_i \sim \mathcal{N}(\mu_{post,i}, \sigma_{post,i}^2) \quad \text{independently for each } i
$$

This samples independently at each location, **breaking** the correlation between points.

**Correct approach (preserves correlation):**

1. Compute Cholesky decomposition: $\Sigma_{post} = L L^T$
2. Sample white noise: $\epsilon \sim \mathcal{N}(0, I)$
3. Generate correlated sample: $y = L \epsilon + \mu_{post}$

The result is a sample from $\mathcal{N}(\mu_{post}, \Sigma_{post})$ with proper spatial correlations.

## Implementation

In `src/core/models.py::posterior_predict()`:

```python
# Posterior covariance
cov_post_emulator = K_test_test - K_test_train @ inv(K_train_train) @ K_train_test

# Mean
mean_post_emulator = ...

# Cholesky decomposition
L = jnp.linalg.cholesky(cov_post_emulator + 1e-10 * I)

# Correlated sample
white_noise = random.normal(rng_key, (n_test,))
sample_post = L @ white_noise + mean_post_emulator

return mean_post_emulator, stdev_post_emulator, sample_post
```

# Prior vs Posterior Predictions

## Methodology from Paper

> N random samples of the uncertain parameters are obtained according to their prior distributions. For each of these samples, a realization is obtained from the conditional multivariate Gaussian distribution based on the **training data (not the observed data)**.

**Key distinction:**

- **Training data** = Simulation outputs (FE model predictions)
- **Observed data** = Experimental measurements

## Prior Predictions

1. Sample hyperparameters from **prior distributions**
2. Condition GP **only on simulation data** (exclude experimental data)
3. Obtain Cholesky-sampled realizations
4. Compute statistics (mean, percentiles) from ensemble

## Posterior Predictions

1. Sample hyperparameters from **posterior distributions** (MCMC)
2. Condition GP on **both simulation and experimental data**
3. Obtain Cholesky-sampled realizations  
4. Compute statistics from ensemble

## Why This Matters

Conditioning on experimental data **reduces uncertainty** in the GP:

$$
\Sigma_{post,with\_exp} < \Sigma_{post,sim\_only}
$$

Therefore:

- **Prior predictions**: Wider bands (less information)
- **Posterior predictions**: Narrower bands (more information from data)

# Summary

**Key principles:**

1. Use Cholesky decomposition to preserve GP spatial correlations
124: 2. Prior predictions condition only on simulation data
125: 3. Posterior predictions condition on both simulation and experimental data
126: 4. Never sample independently at each point - always use Cholesky-sampled realizations

# Prior Predictive Variance at Zero Load

## The Issue

During verification, a discrepancy was observed between the Prior Predictive plots generated by the main analysis script (`analyze.py`) and a standalone verification script (`test_prior_simulation.py`). 

- **Analysis Script**: Showed **zero variance** at Load = 0, with curves starting exactly at the origin (0,0).
- **Verification Script**: Showed **high variance** at Load = 0, with curves fluctuating significantly at the start.

Physically, the extension must be zero when the load is zero ($Ext(Load=0)=0$). Therefore, the variance at Load=0 should be zero.

## The Cause

The discrepancy arose from how the Gaussian Process (GP) was conditioned in the two scripts:

1.  **Analysis Script (`posterior_predict`)**: Implicitly enforced the zero-start condition. When constructing the conditioning set, it effectively included the first test point (Load=0) as a pseudo-observation with value 0. Because the GP conditions on this point, and the correlation between the test point (0,0) and the conditioning point (0,0) is 1.0, the resulting predictive variance is exactly zero.

2.  **Verification Script (`gp_predict_pure`)**: conditioned *strictly* on the provided simulation data. The raw simulation data (`LHS_50`) typically starts at a small non-zero load (e.g., 0.1 kN) and does not explicitly include the (0,0) point. Consequently, the GP treated the region between Load=0 and Load=0.1 as an extrapolation zone driven by the kernel's length scale. With certain prior samples (short length scales), this led to high variance at the unconstrained Load=0 point.

## The Resolution: Explicit Zero-Pinning

To resolve this and align the verification script with physical reality (and the analysis script), we implemented **Explicit Zero-Pinning** in the data preparation implementation (`src/core/models.py`).

**The Decisive Fix:**

For every prior sample, we now dynamically augment the training data passed to the GP:

1.  **Identify Zero-Load Points**: We scan the test grid to find all points where `Load â‰ˆ 0` (using `isclose` for robustness).
2.  **Pin Points**: We add these zero-load points to the training set with a target value of **0.0**.
3.  **Augment Training Set**: The GP conditions on $D_{augmented} = D_{sim} \cup \{(0, \theta_i, y=0)\}$.

By explicitly telling the GP that "for this material $\theta_i$, the extension at Load=0 is 0", we force the predictive distribution to collapse to a point mass at 0 with zero variance. This ensures that all sampled spaghetti curves start exactly at the origin, regardless of the hyperparameters sampled from the prior.
